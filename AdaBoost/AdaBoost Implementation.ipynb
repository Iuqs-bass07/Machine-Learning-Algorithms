{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a02c475",
   "metadata": {},
   "source": [
    "### AdaBoost Implementation\n",
    "\n",
    "AdaBoost is an Boosting algorithm which means that we will take a weak learner model which in this case will be a Decision Stumps and then using those do upsampling and proceed to the consecutive steps\n",
    "\n",
    "### Components\n",
    "\n",
    "#### 1- Weak Learners\n",
    "    This will be a model whose accuracy will be slightly greater than 50%\n",
    "\n",
    "#### 2- Decision Stumps\n",
    "    This will be a decision tree whose maxdepth will be only 1 which means a single split\n",
    "    \n",
    "\n",
    "### Steps Followed\n",
    "    \n",
    "##### 1- Prepare a Decision stump for the given data and calulate the error and the error rate(Alpha)\n",
    "    \n",
    "            error  = summation of misclassified points\n",
    "            error_rate(alpha) = 0.5*log((1-error)/error)\n",
    "    \n",
    "##### 2- Update the weight and then normalize it\n",
    "            \n",
    "            Misclassified\n",
    "                new_wt = wt*exp(alpha)\n",
    "            Correctly Classified\n",
    "                new_wt = wt*exp(-alpha)\n",
    "                \n",
    "##### 3- Creating the range for the dataset\n",
    "\n",
    "                upper_limit = cumsum(normalized_wt)\n",
    "                lower_limit = upper_limit - normalized_wt\n",
    "    \n",
    "##### 4- Create the new Dataset\n",
    "\n",
    "                Generating the new number using random and take the indices in which this lies and there is high chance these will lies in the wider range and the wider ranges will be for misclassified points\n",
    "                \n",
    "                Using those we will create the dataset\n",
    "                \n",
    "##### 5- Inference \n",
    "                The inference will done on those all decision stumps which have been trained and their sign values will be taken\n",
    "            \n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6db96f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c038caa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "class AdaBoost:\n",
    "    \n",
    "    def __init__(self, n_estimators=20):\n",
    "        self.n_estimators = n_estimators\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        \n",
    "        # Initializing weights\n",
    "        weights = np.ones(n_samples) / n_samples\n",
    "        \n",
    "        # To store classifiers\n",
    "        self.clfs = []\n",
    "        \n",
    "        # to store the alpha values for the final predicion\n",
    "        self.alphas = []\n",
    "        \n",
    "        for _ in range(self.n_estimators):\n",
    "            \n",
    "            # Train decision stump\n",
    "            clf = DecisionTreeClassifier(max_depth=1)\n",
    "            clf.fit(X, y, sample_weight=weights)\n",
    "            y_pred = clf.predict(X)\n",
    "            \n",
    "            \n",
    "            # Calculate error and alpha (classifier weight)\n",
    "            error = np.sum(weights * (y_pred != y)) / np.sum(weights)\n",
    "            alpha = 0.5 * np.log((1 - error) / (error + 1e-10)) \n",
    "            \n",
    "            # Update weights and normalize them\n",
    "            for i in range(n_samples):\n",
    "                # Misclassified sample\n",
    "                if y[i] != y_pred[i]:  \n",
    "                    weights[i] *= np.exp(alpha)\n",
    "                else: \n",
    "                    weights[i] *= np.exp(-alpha)\n",
    "            \n",
    "            # Normalize weights\n",
    "            weights /= np.sum(weights)  \n",
    "            \n",
    "            # Store classifier and its alpha\n",
    "            self.clfs.append(clf)\n",
    "            self.alphas.append(alpha)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        # Initialize array to store final predictions\n",
    "        final_pred = np.zeros(X.shape[0])\n",
    "        \n",
    "        # Aggregate predictions from all classifiers\n",
    "        for alpha, clf in zip(self.alphas, self.clfs):\n",
    "            final_pred += alpha * clf.predict(X)\n",
    "        \n",
    "        # Return the sign of the aggregated predictions\n",
    "        return np.sign(final_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c0f6c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f30a07e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16278af4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fced4cbe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c52d46a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc8fb58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab36f50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7078aaf9",
   "metadata": {},
   "source": [
    "### Adaboost implementation for the multiclass Classification\n",
    "\n",
    "Induces the Label Encoding for the Multiclass classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d2e02600",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "class AdaBoost:\n",
    "    def __init__(self, n_estimators=20):\n",
    "        self.n_estimators = n_estimators\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        \n",
    "        # Encode labels to handle multiclass classification using OvR (One-vs-Rest)\n",
    "        self.le = LabelEncoder()\n",
    "        y = self.le.fit_transform(y)\n",
    "        \n",
    "        # Initialize weights uniformly\n",
    "        weights = np.ones(n_samples) / n_samples\n",
    "        \n",
    "        # To store classifiers and their corresponding alpha values\n",
    "        self.clfs = []\n",
    "        self.alphas = []\n",
    "        \n",
    "        for _ in range(self.n_estimators):\n",
    "            # Train weak classifier\n",
    "            clf = DecisionTreeClassifier(max_depth=1)\n",
    "            clf.fit(X, y, sample_weight=weights)\n",
    "            \n",
    "            # Get predictions\n",
    "            y_pred = clf.predict(X)\n",
    "            \n",
    "            # Calculate error and alpha (classifier weight)\n",
    "            error = np.sum(weights * (y_pred != y)) / np.sum(weights)\n",
    "            alpha = 0.5 * np.log((1 - error) / (error + 1e-10))  # Add small value to avoid division by zero\n",
    "            \n",
    "            # Update weights\n",
    "            for i in range(n_samples):\n",
    "                if y[i] != y_pred[i]:  # Misclassified sample\n",
    "                    weights[i] *= np.exp(alpha)\n",
    "                else:  # Correctly classified sample\n",
    "                    weights[i] *= np.exp(-alpha)\n",
    "            \n",
    "            # Normalize weights\n",
    "            weights /= np.sum(weights)\n",
    "            \n",
    "            # Store classifier and its alpha\n",
    "            self.clfs.append(clf)\n",
    "            self.alphas.append(alpha)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        # Initialize array to store final predictions for each class\n",
    "        final_pred = np.zeros((X.shape[0], len(self.le.classes_)))\n",
    "        \n",
    "        # Aggregate predictions from all classifiers\n",
    "        for alpha, clf in zip(self.alphas, self.clfs):\n",
    "            class_pred = clf.predict(X)\n",
    "            for i in range(len(self.le.classes_)):\n",
    "                final_pred[:, i] += alpha * (class_pred == i)\n",
    "        \n",
    "        # Return the class with the highest aggregated score\n",
    "        return self.le.inverse_transform(np.argmax(final_pred, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c4eba3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "import pandas as pd\n",
    "\n",
    "iris = load_iris()\n",
    "df = pd.DataFrame(data=iris.data, columns=iris.feature_names)\n",
    "df['target'] = iris.target "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5f8ac44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the updated AdaBoost model\n",
    "model = AdaBoost(n_estimators=50)  # Increased n_estimators for better performance\n",
    "model.fit(df.iloc[:, :-1], df.iloc[:, -1])\n",
    "\n",
    "y_pred = model.predict(df.iloc[:, :-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b28fb5fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9733333333333334"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy = accuracy_score(df['target'], y_pred)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72863bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999385bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e360bbeb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6cbee73",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
