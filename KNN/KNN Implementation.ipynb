{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32caa6fc",
   "metadata": {},
   "source": [
    "## K-Nearest Neighbors (KNN) Algorithm\n",
    "\n",
    "### Principle Intuition\n",
    "K-Nearest Neighbors (KNN) is a simple, non-parametric, instance-based learning algorithm used for classification and regression tasks. The fundamental idea behind KNN is that given a data point, its class or value can be predicted by looking at the labels of its closest neighbors. KNN assumes that similar points are close to each other in the feature space, which means that nearby points in the training data provide useful information for predicting the label of a new data point.\n",
    "\n",
    "When a new data point needs to be classified:\n",
    "1. The algorithm identifies the **k** closest points from the training set.\n",
    "2. These **k** neighbors \"vote\" by providing their labels, and the new data point is assigned to the class with the majority vote (for classification) or the average of their values (for regression).\n",
    "\n",
    "### Distances Used in KNN\n",
    "The performance of KNN is largely dependent on how distances between data points are measured. Several distance metrics can be used in KNN, with the most common ones being:\n",
    "\n",
    "1. **Euclidean Distance** (L2 Norm):\n",
    "   \\[\n",
    "   d(p, q) = \\sqrt{\\sum_{i=1}^{n}(p_i - q_i)^2}\n",
    "   \\]\n",
    "   This is the most common distance measure, which calculates the straight-line distance between two points in n-dimensional space.\n",
    "\n",
    "2. **Manhattan Distance** (L1 Norm):\n",
    "   \\[\n",
    "   d(p, q) = \\sum_{i=1}^{n} |p_i - q_i|\n",
    "   \\]\n",
    "   This measures the distance between two points as the sum of the absolute differences of their coordinates, often referred to as \"city block distance.\"\n",
    "\n",
    "3. **Minkowski Distance**:\n",
    "   \\[\n",
    "   d(p, q) = \\left( \\sum_{i=1}^{n} |p_i - q_i|^p \\right)^{\\frac{1}{p}}\n",
    "   \\]\n",
    "   This is a generalization of both Euclidean (p=2) and Manhattan (p=1) distances. For any value of **p**, this becomes the Minkowski distance.\n",
    "\n",
    "4. **Cosine Similarity** (used in high-dimensional sparse data):\n",
    "   \\[\n",
    "   d(p, q) = 1 - \\frac{p \\cdot q}{\\|p\\| \\|q\\|}\n",
    "   \\]\n",
    "   Cosine similarity measures the angle between two vectors, making it useful for text and other high-dimensional datasets.\n",
    "\n",
    "### Choosing the Value of k\n",
    "The **k** in KNN represents the number of nearest neighbors to consider when making a prediction. Choosing the right value of **k** is crucial for balancing bias and variance:\n",
    "\n",
    "1. **Small k (e.g., k = 1)**:\n",
    "   - The algorithm becomes sensitive to noise, leading to high variance and overfitting, as it relies on very local information.\n",
    "   - If a single noisy data point is among the nearest neighbors, the prediction can be incorrect.\n",
    "\n",
    "2. **Large k**:\n",
    "   - A large value of **k** smooths out the decision boundary, leading to lower variance but higher bias, which may result in underfitting.\n",
    "   - The algorithm looks at a broader group of neighbors, which can dilute the influence of local data points that are most similar.\n",
    "\n",
    "A common approach is to try several values of **k** and use cross-validation to determine the value that results in the best performance on unseen data.\n",
    "\n",
    "### Overfitting and Underfitting in KNN\n",
    "1. **Overfitting**:\n",
    "   - This happens when the KNN model is too complex and captures noise in the training data, leading to poor generalization to new data.\n",
    "   - Overfitting in KNN occurs when **k** is too small (e.g., **k = 1**), meaning the model becomes too sensitive to the training data, including outliers and noise.\n",
    "\n",
    "2. **Underfitting**:\n",
    "   - Underfitting occurs when the model is too simple to capture the underlying patterns in the data.\n",
    "   - In KNN, this happens when **k** is too large, and the algorithm averages over too many neighbors, which can smooth over important differences between classes or patterns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0264b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b75c444",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Distance_metric:\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    # Euclidean distance\n",
    "    def euclidean_distance(self, vector1, vector2):\n",
    "        distance = 0\n",
    "        for i in range(0, len(vector1)):\n",
    "            distance += (vector1[i] - vector2[i]) ** 2\n",
    "        return distance ** 0.5\n",
    "    \n",
    "    # Manhattan distance\n",
    "    def manhattan_distance(self, vector1, vector2):\n",
    "        distance = 0\n",
    "        for i in range(0, len(vector1)):\n",
    "            distance += abs(vector1[i] - vector2[i])\n",
    "        return distance\n",
    "    \n",
    "    # Minkowski distance\n",
    "    def minkowski_distance(self, vector1, vector2, p=2):\n",
    "        distance = 0\n",
    "        for i in range(0, len(vector1)):\n",
    "            distance += abs(vector1[i] - vector2[i]) ** p\n",
    "        return distance ** (1/p)\n",
    "    \n",
    "    # Hamming distance\n",
    "    def hamming_distance(self, vector1, vector2):\n",
    "        distance = 0\n",
    "        for i in range(0, len(vector1)):\n",
    "            if vector1[i] != vector2[i]:\n",
    "                distance += 1\n",
    "        return distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb3d6e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KNearestNeighbour:\n",
    "    \n",
    "    def __init__(self, k, metric='euclidean_distance'):\n",
    "        self.k = k\n",
    "        self.metric = metric\n",
    "        \n",
    "    # Lazy Learning model, Nothing happens in training\n",
    "    def fit(self, X_train, y_train):\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "    \n",
    "    # Finding all the neighbors of a test row\n",
    "    def KNeighbours(self, test_row, Distance_metric):\n",
    "        cal_dist = Distance_metric()\n",
    "        distances = []\n",
    "        \n",
    "        for i, row in enumerate(self.X_train):\n",
    "            if self.metric == 'euclidean_distance':\n",
    "                distances.append([row, cal_dist.euclidean_distance(test_row, row), self.y_train[i]])\n",
    "            elif self.metric == 'manhattan_distance':\n",
    "                distances.append([row, cal_dist.manhattan_distance(test_row, row), self.y_train[i]])\n",
    "            elif self.metric == 'minkowski_distance':\n",
    "                distances.append([row, cal_dist.minkowski_distance(test_row, row), self.y_train[i]])\n",
    "            elif self.metric == 'hamming_distance':\n",
    "                distances.append([row, cal_dist.hamming_distance(test_row, row), self.y_train[i]])\n",
    "        \n",
    "        # Sort by distance and return the k nearest neighbors\n",
    "        distances.sort(key=lambda x: x[1])\n",
    "        return distances[:self.k]\n",
    "    \n",
    "    # Inference of the X_test by using K neighbor majority count\n",
    "    def predict(self, X_test, Distance_metric):\n",
    "        predictions = []\n",
    "        \n",
    "        for row in X_test:\n",
    "            neighbors = self.KNeighbours(row, Distance_metric)\n",
    "            output_labels = [label[-1] for label in neighbors]\n",
    "            pred = max(set(output_labels), key=output_labels.count)\n",
    "            predictions.append(pred)\n",
    "            \n",
    "        return predictions\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb2cac2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
