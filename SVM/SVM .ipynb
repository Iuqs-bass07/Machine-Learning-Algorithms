{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "114d3296",
   "metadata": {},
   "source": [
    "### SVM\n",
    "\n",
    "The principle of SVM is that if there exists a hyperplane that classifies the data into two regions, there can be multiple hyperplanes capable of separating the data. SVM finds the hyperplane that has the largest margin, maximizing the distance between the hyperplane and the nearest points (support vectors) from both classes. This is known as **Maximizing the Margin**.\n",
    "\n",
    "The idea is that a larger margin leads to better generalization on unseen data. The goal of SVM is to minimize this margin distance (D) while ensuring proper classification of the data points. This works under the concept of **Margin Maximization**.\n",
    "\n",
    "### Margin Distance\n",
    "\n",
    "The margin is the distance between the positive hyperplane and the negative hyperplane that touch the support vectors of each respective region. The larger the margin, the more robust the classifier is to noise and errors.\n",
    "\n",
    "### Support Vector\n",
    "\n",
    "Support vectors are the data points that lie closest to the decision boundary (hyperplane). These points play a crucial role in defining the position of the hyperplane. If the hyperplane is moved, keeping the same slope, the first points it meets in both positive and negative regions are the support vectors. These vectors are critical as they directly influence the margin and the decision boundary of the SVM.\n",
    "\n",
    "### 1- Hard Margin SVM\n",
    "\n",
    "This applies to linearly separable data where a clear hyperplane divides the data into two regions with no misclassifications. The goal is to maximize the margin between the two classes.\n",
    "\n",
    "**Loss Function:**\n",
    "\\[\n",
    "\\text{Loss function} = \\arg \\min \\left(\\frac{2}{\\|\\mathbf{w}\\|}\\right)\n",
    "\\]\n",
    "\n",
    "Subject to:\n",
    "\\[\n",
    "y_i(\\mathbf{w} \\cdot \\mathbf{x}_i + b) \\geq 1\n",
    "\\]\n",
    "\n",
    "Where \\(\\mathbf{w}\\) is the weight vector, \\(b\\) is the bias, and \\(y_i\\) are the class labels.\n",
    "\n",
    "### 2- Soft Margin SVM\n",
    "\n",
    "This is used when the data is not perfectly linearly separable, meaning some misclassifications are inevitable. There is a tradeoff between maximizing the margin and minimizing classification errors. Here, we introduce slack variables \\(\\xi_i\\) to allow for misclassification, which penalizes the model based on how far the points are from the correct margin.\n",
    "\n",
    "**Loss Function:**\n",
    "\\[\n",
    "\\text{Loss function} = \\arg \\min \\left(\\frac{2}{\\|\\mathbf{w}\\|}\\right) + C \\sum \\xi_i\n",
    "\\]\n",
    "\n",
    "Where \\(C\\) is the regularization parameter, controlling the tradeoff between maximizing the margin and minimizing the classification error, and \\(\\xi_i\\) represents the slack variables.\n",
    "\n",
    "### Kernel Trick\n",
    "\n",
    "Some datasets are not linearly separable, even with a soft margin. In such cases, the **Kernel Trick** is used. The kernel function transforms the data into a higher-dimensional space where a hyperplane can be used to separate the classes. Instead of explicitly transforming the data, kernel functions compute the dot products in the higher-dimensional space efficiently, allowing SVM to work in this transformed space without high computational cost.\n",
    "\n",
    "### Types of Kernel Functions\n",
    "\n",
    "1. **RBF (Radial Basis Function):** \n",
    "   \\[\n",
    "   K(\\mathbf{x}, \\mathbf{x'}) = \\exp\\left(-\\gamma \\|\\mathbf{x} - \\mathbf{x'}\\|^2\\right)\n",
    "   \\]\n",
    "   \n",
    "   This kernel is widely used and maps the data to an infinite-dimensional space. It works well when the relationship between the data points is non-linear.\n",
    "   \n",
    "   \n",
    "\n",
    "2. **Polynomial Kernel:** \n",
    "   \\[\n",
    "   K(\\mathbf{x}, \\mathbf{x'}) = (\\mathbf{x} \\cdot \\mathbf{x'} + c)^d\n",
    "   \\]\n",
    "   \n",
    "   This kernel allows learning of polynomial decision boundaries of degree \\(d\\).\n",
    "\n",
    "\n",
    "3. **Sigmoid Kernel:** \n",
    "   \\[\n",
    "   K(\\mathbf{x}, \\mathbf{x'}) = \\tanh(\\alpha \\mathbf{x} \\cdot \\mathbf{x'} + c)\n",
    "   \\]\n",
    "\n",
    "    This kernel behaves like a neural network's activation function, though it may not always satisfy the conditions for being a proper kernel.\n",
    "\n",
    "\n",
    "4. **Linear Kernel:** \n",
    "   \\[\n",
    "   K(\\mathbf{x}, \\mathbf{x'}) = \\mathbf{x} \\cdot \\mathbf{x'}\n",
    "   \\]\n",
    "       \n",
    "    This is used when the data is linearly separable and is the simplest kernel function.\n",
    "\n",
    "\n",
    "5. **Laplacian Kernel:** \n",
    "   \\[\n",
    "   K(\\mathbf{x}, \\mathbf{x'}) = \\exp\\left(-\\frac{\\|\\mathbf{x} - \\mathbf{x'}\\|}{\\sigma}\\right)\n",
    "   \\]\n",
    "   \n",
    "   This kernel is similar to the RBF kernel but uses the \\(L_1\\) norm instead of the \\(L_2\\) norm, making it sensitive to absolute distances.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba42eb9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "762b5873",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVM:\n",
    "    \n",
    "    def __init__(self, learning_rate = 0.01, lamda = 0.01, n_iters = 1000):\n",
    "        \n",
    "        self.learning_rate = learning_rate\n",
    "        self.lamda = lamda\n",
    "        self.n_iters = n_iters\n",
    "        self.w = None\n",
    "        self.b = None\n",
    "    \n",
    "    # fit function will work on the condition where it lies \n",
    "    ### 1-- Correctly classified -- (xi) term will be zero\n",
    "    ### 2-- Misclassified -- (xi) term will not be zero\n",
    "    \n",
    "    ## gradient will change according to the function of t each of the x vector\n",
    "    def fit(self, X_train, y_train):\n",
    "        \n",
    "        n_samples, n_features = X.shape\n",
    "        \n",
    "        #changing it into negative and positive hyperplane\n",
    "        y = np.where(y <= 0, -1, 1)\n",
    "        \n",
    "        self.w = np.ones(n_features)\n",
    "        self.b = 0\n",
    "        \n",
    "        for _ in range(self.n_iters):\n",
    "            \n",
    "            for idx, x in enumerate(X_train):\n",
    "                \n",
    "                condition = y_train[idx]*(np.dot(x, self.w) + self.b) >= 1\n",
    "                \n",
    "                ### 1-- Correctly classified\n",
    "                if condition:\n",
    "                    \n",
    "                    self.w -= self.learning_rate*(2 * lamda * self.w)\n",
    "                \n",
    "                else:    ### 2-- Misclassified\n",
    "                    self.w -= self.learning_rate*(2*lamda*self.w - np.dot(x, y_train[idx]))\n",
    "                    self.b -= self.learning_rate * y_train[idx]\n",
    "    \n",
    "    \n",
    "    #inference function ---> calculate where the vector lies in which hyperplane\n",
    "    \n",
    "    #1-- if the value is positive then 1 -- positive hyper plane\n",
    "    #2-- if the value is negative then 0 -- negative hyper plane\n",
    "    \n",
    "    def predict(self, X_test):\n",
    "        val = np.dot(X_test, self.w) + self.b\n",
    "        \n",
    "        return np.sign(val)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d171b9ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
