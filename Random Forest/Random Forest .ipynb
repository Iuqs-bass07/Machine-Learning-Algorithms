{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06298f3a",
   "metadata": {},
   "source": [
    "### Random Forest Implementation\n",
    "\n",
    "Random forest is a bagging algorithm which is built on the top of decision tree. Bagging name comes from the \n",
    "\n",
    "#### B- Bootstrap\n",
    "This means that the sampling of the whole dataset is done and the then the samples data is fed to the model. Each model is trained on a different set of sampled data\n",
    "\n",
    "#### Agg- Aggregate\n",
    "This means that the whole set of models are train on a different set of data and then while making their inference the aggregate of the result has been taken\n",
    "\n",
    "\n",
    "#### Components\n",
    "\n",
    "##### Decision Tree Class:\n",
    "Constructs individual decision trees based on specified parameters like minimum samples and maximum depth.\n",
    "\n",
    "##### Random Forest Function:\n",
    "Builds a random forest using multiple decision trees trained on bootstrap samples.\n",
    "\n",
    "##### Prediction Function:\n",
    "Aggregates predictions from all decision trees in the forest to provide the final prediction using a majority vote approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95eddd3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0e23d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTree:\n",
    "    \n",
    "    # constructor of the class DecisionTree\n",
    "    def __init__(self, min_samples_left=2, max_depth=2):\n",
    "        self.root = None\n",
    "        self.min_samples_left = min_samples_left\n",
    "        self.max_depth = max_depth\n",
    "\n",
    "    \n",
    "    ## function to build the decision tree\n",
    "    def BuildTree(self, dataset, curr_depth=0):\n",
    "        X, y = dataset[:, :-1], dataset[:, -1]\n",
    "        num_samples = X.shape[0]\n",
    "        num_features = X.shape[1]\n",
    "        \n",
    "        #splitting on the best feature\n",
    "        if num_samples >= self.min_samples_left and curr_depth <= self.max_depth:\n",
    "            best_split = self.get_best_split(dataset, num_samples, num_features)\n",
    "            if best_split[\"info_gain\"] > 0:\n",
    "                left_subtree = self.BuildTr(best_split[\"dataset_left\"], curr_depth + 1)\n",
    "                right_subtree = self.BuildTree(best_split[\n",
    "                    \"dataset_right\"], curr_depth + 1)\n",
    "                return Node(best_split[\"feature_index\"], best_split[\"threshold\"], left_subtree, right_subtree, best_split[\"info_gain\"])\n",
    "\n",
    "        leaf_value = self.calculate_leaf_value(y)\n",
    "        return Node(value=leaf_value)\n",
    "    \n",
    "    # function to get the best split\n",
    "    def get_best_split(self, dataset, num_samples, num_features):\n",
    "        best_split = {}\n",
    "        max_info_gain = -float(\"inf\")\n",
    "\n",
    "        for feature_index in range(num_features):\n",
    "            feature_values = dataset[:, feature_index]\n",
    "            possible_threshold = np.unique(feature_values)\n",
    "\n",
    "            for threshold in possible_threshold:\n",
    "                dataset_left, dataset_right = self.split(dataset, feature_index, threshold)\n",
    "                if len(dataset_left) > 0 and len(dataset_right) > 0:\n",
    "                    y, left_y, right_y = dataset[:, -1], dataset_left[:, -1], dataset_right[:, -1]\n",
    "                    curr_info_gain = self.information_gain(y, left_y, right_y, \"gini\")\n",
    "                    if curr_info_gain > max_info_gain:\n",
    "                        best_split[\"feature_index\"] = feature_index\n",
    "                        best_split[\"threshold\"] = threshold\n",
    "                        best_split[\"dataset_left\"] = dataset_left\n",
    "                        best_split[\"dataset_right\"] = dataset_right\n",
    "                        best_split[\"info_gain\"] = curr_info_gain\n",
    "                        max_info_gain = curr_info_gain\n",
    "\n",
    "        return best_split\n",
    "\n",
    "    #split function to divide the data on a basis of a threshold of a feature\n",
    "    def split(self, dataset, feature_index, threshold):\n",
    "        dataset_left = np.array([row for row in dataset if row[feature_index] <= threshold])\n",
    "        dataset_right = np.array([row for row in dataset if row[feature_index] > threshold])\n",
    "        return dataset_left, dataset_right\n",
    "\n",
    "    #information gain\n",
    "    # 1- entropy\n",
    "    # 2- gini impurity\n",
    "    \n",
    "    def information_gain(self, y, left_y, right_y, mode=\"entropy\"):\n",
    "        weight_l = len(left_y) / len(y)\n",
    "        weight_r = len(right_y) / len(y)\n",
    "\n",
    "        if mode == \"gini\":\n",
    "            gain = self.gini_impurity(y) - (weight_l * self.gini_impurity(left_y) + weight_r * self.gini_impurity(right_y))\n",
    "        else:\n",
    "            gain = self.entropy(y) - (weight_l * self.entropy(left_y) + weight_r * self.entropy(right_y))\n",
    "\n",
    "        return gain\n",
    "\n",
    "    def entropy(self, y):\n",
    "        labels = np.unique(y)\n",
    "        entropy = 0\n",
    "        for cls in labels:\n",
    "            p_class = len(y[y == cls]) / len(y)\n",
    "            entropy += -p_class * np.log2(p_class)\n",
    "        return entropy\n",
    "\n",
    "    def gini_impurity(self, y):\n",
    "        labels = np.unique(y)\n",
    "        gini = 0\n",
    "        for cls in labels:\n",
    "            p_class = len(y[y == cls]) / len(y)\n",
    "            gini += p_class ** 2\n",
    "        return 1 - gini\n",
    "\n",
    "    def calculate_leaf_value(self, y):\n",
    "        y = list(y)\n",
    "        return max(y, key=y.count)\n",
    "    \n",
    "    #model training\n",
    "    def fit(self, X, y):\n",
    "        data = np.concatenate((X, y.reshape(-1, 1)), axis=1)\n",
    "        self.root = self.BuildTree(data)\n",
    "\n",
    "    def predict(self, X):\n",
    "        predictions = [self.make_prediction(x, self.root) for x in X]\n",
    "        return predictions\n",
    "\n",
    "    #making model inference\n",
    "    def make_prediction(self, x, tree):\n",
    "        if tree.value is not None:\n",
    "            return tree.value\n",
    "        feature_val = x[tree.feature_index]\n",
    "        if feature_val <= tree.threshold:\n",
    "            return self.make_prediction(x, tree.left)\n",
    "        else:\n",
    "            return self.make_prediction(x, tree.right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a4ff7106",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Bootstrap_sample(data, bootstrap_size):\n",
    "    random_index = np.random.randint(0, data.shape[0], size = bootstrap_size)\n",
    "    \n",
    "    return data.iloc[random_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56bd98a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Random_Forest(data, bootstrap_size, random_attributes, random_splits, forest_size=20, tree_depth=1000):\n",
    "    forest = []\n",
    "    for i in range(forest_size):\n",
    "        bootstrap_data = Bootstrap_sample(data, bootstrap_size)\n",
    "        \n",
    "        # Create an instance of the DecisionTree with the required parameters\n",
    "        dtree = DecisionTree(min_samples_left=2, max_depth=tree_depth)\n",
    "        decision_tree = dtree.BuildTree(bootstrap_data.values, curr_depth=0)\n",
    "        forest.append(decision_tree)\n",
    "    return forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ae53e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RandomForestPredictions(dataFrame, randomForest):\n",
    "    predictions = {}\n",
    "    for i, tree in enumerate(randomForest):\n",
    "        column = f\"decision tree {i}\"\n",
    "        # Ensure the method to predict from the DecisionTree class is correctly called\n",
    "        predictions[column] = [tree.predict(row) for _, row in dataFrame.iterrows()]\n",
    "    predictions_df = pd.DataFrame(predictions)\n",
    "    return predictions_df.mode(axis=1)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47fe1979",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
